{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return False;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return False;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference for Neural Neworks\n",
    "\n",
    "\n",
    "### Bayesian Inference?\n",
    "Often when we train a model, we have a cost function constructed from, say taking the difference of network output and the desired output in the training set. The result of the optimisation is a minimised cost function at a *single* set of model parameters. Bayesian inference aims to get a **distribution** of the model parameters instead (generally obtaining a posterior distribution). This distribution of model parameters conditioned on the training data is known as the *posterior*. With Bayes rule, we can write the model parameter posterior distribution in terms of the likelihood probabilty produced by the model, a prior distribution on the model parameter that captures what we think it should be, and the marginal-likelihood. This is similar to the latent variable posterior in the Variational Autoencoder notebook, but very different in concept as we train the model to get a distibution of the model paramters rather than a point estimate. When we have a distribution of parameters, we can tell how well trained the model is by looking at the variance of the parameter distribution. A high variance means there is lot of uncertaincy in the model and a sharp mode indicates the model parameters are well set. Large model parameter variance does not always lead to large output variance as the variance of the model parameters are transfored to produce the output (same input, output variance as we sample parameters from the posterior). \n",
    "\n",
    "Generally we can write the folloing relation between the posterior and likelihood according to Bayes formula:\n",
    "\n",
    "\n",
    "$$\n",
    "p(\\textbf{$\\omega$}|\\textbf{X}_{tr}, \\textbf{Y}_{tr}) = \n",
    "\\frac\n",
    "{p(y|\\textbf{$\\omega$}, \\textbf{X}_{tr}, \\textbf{Y}_{tr})p(\\textbf{$\\omega$})}\n",
    "{p(\\textbf{Y}_{tr}|\\textbf{X}_{tr})}\n",
    "$$\n",
    "\n",
    "Here $\\omega$ are the model parameters and $X_{tr}$ and $Y_{tr}$ are inputs/outputs of training examples of a task (e.g. classification). In some simple cases we can analytically calculate the posterior from the likelihood and prior. (see [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)). For neural network classification the likelihood $p(y|\\textbf{$\\omega$}, \\textbf{X}_{tr}, \\textbf{Y}_{tr})$ is the product (or sum for log likelihood) of computed probability of all training set examples (or all examples within a minibatch).\n",
    "\n",
    "\n",
    "### So how to do Bayesian Inference on Neural Networks?\n",
    "For neural networks (e.g. classification), we have the likelihood function computed from softmax of the network outputs. These network outputs are computed by folloing the activation equations with a large number of network weights. Computing an expression for the posterior from the likelihood function is thus intractable in any practical case (similar situation for variational autoencoders). The marginal likelihood is as with the general case - very hard to compute if try marginalising out all the weights from the likelihood. One way to tackle this is to use Variational Inference - i.e. assume a manageable form of the posterior (called Variational Distribution). This manageable form is parameterised (parameter $\\theta$) and we use optimisation to find the optimal parameter. This is best illustrated with an example. Suppose we define a variational distribution to approximate the true posterior:\n",
    "\n",
    "$$\n",
    "q(\\omega|\\theta) \\approx\n",
    "\\frac\n",
    "{p(y|\\omega, \\textbf{X}_{tr}, \\textbf{Y}_{tr})p(\\omega)}\n",
    "{Z}\n",
    "$$\n",
    "\n",
    "we're writing the marginal likelihood $Z=p(\\textbf{Y}_{tr}|\\textbf{X}_{tr})$ for simplicity and $\\theta$ is the parameter of the variational distribution $q(\\omega|\\theta)$ that we can tune to achieve a better approximation of the posterior. To measure how well the approximation is carried out, we can use the KL divergence between $q(\\omega|\\theta)$ and the true posterior:\n",
    "\n",
    "$$\n",
    "KL \\left (\n",
    "q(\\omega|\\theta) \\bigg| \\bigg|\n",
    "\\frac\n",
    "{p(y|\\omega, \\textbf{X}_{tr}, \\textbf{Y}_{tr})p(\\omega)}\n",
    "{Z}\n",
    "\\right)\\\\\n",
    "=\n",
    "\\int q(\\omega|\\theta) log q(\\omega|\\theta) d\\omega - \n",
    "\\int q(\\omega|\\theta) log\\left[ \\frac\n",
    "{p(y|\\omega, \\textbf{X}_{tr}, \\textbf{Y}_{tr})p(\\omega)}\n",
    "{Z}\\right] d\\omega \\\\\n",
    "=\n",
    "\\int q(\\omega|\\theta) log \\left[ \\frac{q(\\omega|\\theta)}{p(\\omega)} \\right] d\\omega -\n",
    "\\int q(\\omega|\\theta) log \\left[ p(y|\\omega, \\textbf{X}_{tr}, \\textbf{Y}_{tr}) \\right] d\\omega\n",
    "+ \\text{ constant}\n",
    "$$\n",
    "\n",
    "The left term is the KL divergence between the variational distribution and the prior whereas the right term is the expectation of the log likelihood and can be approximated by MC sample mean with samples of the network weights $\\omega$ drawn from the variational distribution $q(\\omega|\\theta)$. We can find the parameters $\\theta$ that minimises the above expression (i.e. parameters where the variational distribution is at it's best approximation for the true posterior). i.e. \n",
    "\n",
    "$$\n",
    "arg\\_min_{\\theta} \\left\\{ KL \\left( q(\\omega|\\theta) || p(\\omega) \\right) + \n",
    "\\left< log \\left[ p(y|\\omega, \\textbf{X}_{tr}, \\textbf{Y}_{tr}) \\right] \\right>\\bigg|_{\\omega \\sim q(\\omega|\\theta)}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Under this condition, we maximise the likelihood with a regularlisation term that originates from $KL \\left( q(\\omega|\\theta) || p(\\omega) \\right)$. For the neural network we an choose multivariant gaussians as the variational distribution and prior. i.e.\n",
    "$p(\\omega) = \\mathcal{N}(0, \\mathcal{I})$ and \n",
    "$q(\\omega|\\theta) = \\mathcal{N}(\\mu, \\Sigma)$, with $\\theta=(\\mu, \\Sigma)$ being the mean and covariance matrix parameters}. The KL divergence of the two multivariant gaussians are given by:\n",
    "\n",
    "$$\n",
    "KL\\left( p(\\omega) || q(\\omega|\\theta) \\right) = \n",
    "\\frac{1}{2}\\left\\{ Tr(\\Sigma) + \\mu^T\\mu - k - log|\\Sigma| \\right\\}\n",
    "$$\n",
    "\n",
    "This includes \"L2 regularisation\" terms on the mean vector and the diagonal elements of the covariance matrix of the variational distribution. This combined with the log likelihood, is the cost function we're going to feed into the optimisation routine. How do we compute the gradient of the cost with respect to our parameters $\\theta=(\\mu, \\Sigma)$? Afterall, we generate some random weights, use these weights to compute the average log likeliood, and expect to compute gradient of the log likelihood w.r.t the weights. We can do this easily if the random weights are generated using a parameter free generator and some linear algebra (known as the \"re-parameterisation\" trick), where each sample of the weights is presented as:\n",
    "\n",
    "$$\n",
    "w_i = \\mu_i + L_i\\epsilon \\text{, with } \\epsilon \\sim \\mathcal{N}(0, \\mathcal{I})\n",
    "\\text{ and } \n",
    "L_iL_i^T = \\Sigma_i \\text{ being the covariance matrix of vector } w_i\n",
    "$$\n",
    "\n",
    "This is a stardard way of generating a general multivariant gaussian via transforming samples from a isotropic, unit variance and zero mean gaussian distribution. The resulting mean of the transformed variables is given by $\\mu_i$ and the covariance matrix is given by $L_iL_i^T$. In the code we force $L_i$ to be a lower triangular matrix [such that](https://en.wikipedia.org/wiki/Cholesky_decomposition) we have a positive definite covariance matrix. The resulting weights, are thus differentiable w.r.t parameters $\\mu_i$ and $\\Sigma_i$ ($L_i$). We've also separated out each neural network layer $i$, implicitely setting the network weights between layers to be independent of each other. In the objective function above the regularisation term is derived with respective single prior $p(\\omega)$ and variational distribution $q(\\omega|\\theta)$. If we assume the combined variational distribution of the weights defining each neural network layer is also gaussian. The overall variational distribution is characterised by:\n",
    "\n",
    "$$\n",
    "\\mu = [\\mu_1, \\mu_2, ..\\mu_n]\\\\\n",
    "\\Sigma = \n",
    "\\begin{bmatrix}\n",
    "    \\Sigma_1 & 0 & 0 & \\dots  & 0 \\\\\n",
    "    0 & \\Sigma_2 & 0 & \\dots  & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots  & \\Sigma_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The determinant of a block matrix is given by the [product of the individual blocks](https://en.wikipedia.org/wiki/Determinant#Block_matrices). The log determinant required in the regularisation term above can then be computed as sum of log determinant of each invividual block. $KL\\left( p(\\omega) || q(\\omega|\\theta) \\right)$, the overall regularisation term for the entire network is thus simply the sum of regularisation terms of each layer. \n",
    "\n",
    "Note - Adding noise to neural network weights is connected to variational bayes approximation of the weights posterior of a neural network. The difference is with a Variational Bayesian approach we first optimise for the parameter of the weights' distribution rather than on the weights, whereas adding noise with a fixed noise parameter we're operating on a sub-optimal variational distribution. In other words, for Variational Bayes, the noise parameter is also included in the optimisation process. \n",
    "\n",
    "We train a multilayer perceptron with gaussian weights below. Because each gradient update is calculated based on stochastic weight samples, the network is substantially harder to train than one with deterministic weights. Like any other MC sampling based gradient descent optimisation - learning rate, regularisation vs likelihood ratio, initialisation, number of mc samples and input value range are all critical for successful network training. Some literature advocates a single sample per mini batch element, this could be data/network dependent. More reliable ways to tackle gradient noise (e.g. [variance reduction](https://arxiv.org/abs/1705.07880)) are reported in literature. Here we use a large number of MC samples to smoothout the gradient updates. \n",
    "\n",
    "Once network is trained, we can get some network confidence, show some stats of the variation ratio, even a histogram over the test set as well as your accuracy/precision + recall stuff - these are calculated from sampling as well as we have to approximate the marginalisation in the inference stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [1. 1.]\n",
      "recall: [1. 1.]\n",
      "fbeta_score: [1. 1.]\n",
      "test data split: [1 2]\n"
     ]
    }
   ],
   "source": [
    "# simple multilayer perceptron with gaussian variational distribution\n",
    "# wrapping all network graphs in a class\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class MCMLP:\n",
    "    \"\"\"\n",
    "    multi layer perceptron with random weights - for variational bayes training\n",
    "    variational distribution of network weights are gaussian with isotropic gaussian priors\n",
    "    \n",
    "    tuning - set eps to zero, see it converge, then solve the variance problem separately\n",
    "           - very sensitive to learning rate in MC mode. \n",
    "           - initialisation is crucial\n",
    "           - training algorithm must have some \"averaging\" and \"momentum\" feature\n",
    "           - inputs best to be scaled to a common range across features. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, neu_list, n_mc, activation=tf.nn.elu, guard=10**-6,\n",
    "                 learning_rate=0.01, mc=True):\n",
    "        \"\"\"\n",
    "        constructs the graph of MLP\n",
    "        \n",
    "        n_in: input size\n",
    "        neu_list: list dictating how many neurons for each layer\n",
    "        n_mc: number of mc samples for each run\n",
    "        activation: tensorflow activation functions\n",
    "            e.g. tf.nn.elu, tf.nn.sigmoid\n",
    "            for hidden layers only. output layer uses sigmoid\n",
    "        mc: bool\n",
    "            do we have stochastic weights? disable to fine tune network parameters \n",
    "            with noise free gradients\n",
    "        \"\"\"\n",
    "        assert len(neu_list) > 1\n",
    "        self.n_in = n_in\n",
    "        self.neu_list = neu_list\n",
    "        self.in_list = [n_in] + neu_list[:-1]\n",
    "        self.n_mc = n_mc\n",
    "        self.guard = tf.constant(guard, dtype=tf.float32)\n",
    "        \n",
    "        # input/output variables\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, n_in))\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=(None, 1))               \n",
    "        \n",
    "        # holding variables of all layers in lists\n",
    "        self.mu = []  # means\n",
    "        self.tiled_mu = [] # tiled mean for convenience\n",
    "        self.sig_sqrt = []  # square root of covariance matrices\n",
    "        self.sig = []  # covariance matrices\n",
    "        self.eps = []  # random ~N(0, 1) for mc sampling\n",
    "        self.weights = []  # actual weights for MLP\n",
    "        self.weights_mb = []  # transposed weights for mb\n",
    "        self.hidden = [] # output of each layer\n",
    "        self.kl = []  # KL(variational_dist||prior) = KL( N(mu,sig) || N(0,I) ) of each layer weights\n",
    "        self.l1 = []  # l1 cost of weights and bias\n",
    "        self.bias = []  # bias of neurons. non-stochastic\n",
    "        \n",
    "        for n, (l_in, l_neu) in enumerate(zip(self.in_list, self.neu_list)):\n",
    "            self.mu.append(\n",
    "                tf.Variable(initial_value=tf.random_normal(shape=(l_neu*l_in, 1)))\n",
    "            )             \n",
    "            self.sig_sqrt.append(\n",
    "                tf.matrix_band_part(\n",
    "                    tf.Variable(initial_value=tf.random_uniform(shape=(l_neu*l_in, l_neu*l_in), \n",
    "                                                                minval=10**-6, maxval=1.0)),\n",
    "                    -1, 0)\n",
    "            )\n",
    "            self.sig.append(\n",
    "                tf.matmul(self.sig_sqrt[-1], tf.transpose(self.sig_sqrt[-1]))\n",
    "            )\n",
    "            \n",
    "            if mc is True:\n",
    "                self.eps.append(tf.random_normal(shape=(l_neu*l_in, n_mc)))\n",
    "            else:\n",
    "                self.eps.append(tf.zeros(shape=(l_neu*l_in, n_mc)))  # set eps to zero -> check if we have variance issues in MC\n",
    "                \n",
    "            self.tiled_mu.append(\n",
    "                tf.matmul(self.mu[-1], tf.ones(shape=(1, n_mc)))\n",
    "            )\n",
    "            self.weights.append(\n",
    "                self.tiled_mu[-1] +  tf.matmul(self.sig_sqrt[-1], self.eps[-1])\n",
    "            )\n",
    "            self.weights_mb.append(\n",
    "                tf.reshape(tf.transpose(self.weights[-1], [1, 0]), [n_mc, l_in, -1])\n",
    "            )\n",
    "            self.bias.append(\n",
    "                tf.Variable(initial_value=tf.random_normal(shape=(1, l_neu)))\n",
    "            )\n",
    "            self.l1.append(\n",
    "                tf.reduce_sum(tf.abs(self.weights[-1])) + tf.reduce_sum(tf.abs(self.bias[-1]))\n",
    "            )\n",
    "            self.kl.append(\n",
    "                tf.trace(self.sig[-1]) + tf.matmul(tf.transpose(self.mu[-1]), self.mu[-1]) - l_neu*l_in -\n",
    "                tf.linalg.slogdet(self.sig[-1])[1] + tf.matmul(self.bias[-1], tf.transpose(self.bias[-1]))\n",
    "            )\n",
    "            if n == 0:\n",
    "                self.hidden.append(\n",
    "                    activation(tf.matmul(tf.stack([self.x]*n_mc, axis=0), self.weights_mb[-1]) + self.bias[0])\n",
    "                )  # input layer needs tiling self.x\n",
    "            elif n == len(neu_list) - 1:\n",
    "                self.hidden.append(\n",
    "                    tf.sigmoid(tf.matmul(self.hidden[-1], self.weights_mb[-1]) + self.bias[-1])\n",
    "                )  # last layer sigmoid\n",
    "            else:\n",
    "                self.hidden.append(\n",
    "                    activation(tf.matmul(self.hidden[-1], self.weights_mb[-1]) + self.bias[-1])\n",
    "                )  # hidden layers we use activation specified\n",
    "                \n",
    "        self.output_p = tf.reduce_mean(self.hidden[-1], axis=0)  # [mb_size, 1], mc averaged\n",
    "        \n",
    "        self.l_likelihood = tf.reduce_sum(  # across MB components\n",
    "            tf.log(tf.maximum(self.guard, \n",
    "                              tf.multiply(self.y, self.output_p) + \n",
    "                              tf.multiply((tf.ones_like(self.y) - self.y), \n",
    "                                          (tf.ones_like(self.output_p) - self.output_p)))))\n",
    "        \n",
    "        self.kl_sum = tf.add_n(self.kl)\n",
    "        self.l1_sum = tf.add_n(self.l1)\n",
    "        self.cost = - self.l_likelihood + 0.001*self.kl_sum\n",
    "\n",
    "        self.optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1, beta2=0.1)\n",
    "        self.training_op = self.optimiser.minimize(self.cost)\n",
    "        \n",
    "        # evaluating output\n",
    "        self.outputs_pn = tf.sign(self.output_p - tf.random_uniform(tf.shape(self.output_p), minval=0, maxval=1))  # {-1, +1}\n",
    "        self.output = tf.minimum(tf.maximum(self.outputs_pn, tf.constant(0.0, dtype=tf.float32)), tf.constant(1.0, dtype=tf.float32))  # {0, 1}\n",
    "            \n",
    "n_in = 4 # input size\n",
    "n_mb = 3 # mini batch size\n",
    "n_mc = 2000 # number of mc samples per input array\n",
    "\n",
    "LAYER = 1  # which layer to print out in this experiment\n",
    "    \n",
    "# simple test run with toy data\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    mc_mlp = MCMLP(n_in, [5, 3, 1], n_mc, learning_rate=0.05)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for n in range(200):\n",
    "        (mu_, tiled_mu_, bias_, sig_sqrt_, sig_,\n",
    "         eps_, weights_, weights_mb_, hidden_, kl_, output_p_,\n",
    "         l_likelihood_, kl_sum_, cost_,\n",
    "         training_op_,\n",
    "         output_) = sess.run([mc_mlp.mu[LAYER], mc_mlp.tiled_mu[LAYER], mc_mlp.bias[LAYER],\n",
    "                                                     mc_mlp.sig_sqrt[LAYER], mc_mlp.sig[LAYER],\n",
    "                                                     mc_mlp.eps[LAYER], mc_mlp.weights[LAYER],\n",
    "                                                     mc_mlp.weights_mb[LAYER], mc_mlp.hidden[LAYER],\n",
    "                                                     mc_mlp.kl[LAYER],\n",
    "                                                     mc_mlp.output_p, mc_mlp.l_likelihood, \n",
    "                                                     mc_mlp.kl_sum, mc_mlp.cost,\n",
    "                                                     mc_mlp.training_op,\n",
    "                                                     mc_mlp.output],\n",
    "                                                    {mc_mlp.x: np.array([[1, 1, 1, 1],\n",
    "                                                                         [2, 2, 2, 2],\n",
    "                                                                         [3, 3, 3, 3]],dtype=np.float32),\n",
    "                                                    mc_mlp.y: np.array([[1], [0], [1]], dtype=np.float32)})\n",
    "#     print('likelihood: {}, KL term: {}, Overall cost: {}'.format(l_likelihood_, kl_sum_, cost_))\n",
    "    # evaluate\n",
    "    x_test_arr =  np.array([[1, 1, 1, 1], \n",
    "                            [2, 2, 2, 2], \n",
    "                            [3, 3, 3, 3]],dtype=np.float32)\n",
    "    y_test_arr = np.array([[1], [0], [1]], dtype=np.float32)\n",
    "    test_output_ = sess.run(mc_mlp.output, {mc_mlp.x: x_test_arr})\n",
    "    (precision, recall, \n",
    "     fbeta_score, split) = precision_recall_fscore_support(y_test_arr, test_output_)\n",
    "    print('precision: {}'.format(precision))\n",
    "    print('recall: {}'.format(recall))\n",
    "    print('fbeta_score: {}'.format(fbeta_score))\n",
    "    print('test data split: {}'.format(split))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.619131</td>\n",
       "      <td>2.935269</td>\n",
       "      <td>4.489174</td>\n",
       "      <td>6.363185</td>\n",
       "      <td>-2.350210</td>\n",
       "      <td>-2.380518</td>\n",
       "      <td>-3.092911</td>\n",
       "      <td>-3.553578</td>\n",
       "      <td>-1.719430</td>\n",
       "      <td>-2.773718</td>\n",
       "      <td>-1.065554</td>\n",
       "      <td>0.104874</td>\n",
       "      <td>0.889575</td>\n",
       "      <td>3.379864</td>\n",
       "      <td>-5.028600</td>\n",
       "      <td>-3.879864</td>\n",
       "      <td>-3.958432</td>\n",
       "      <td>-4.741481</td>\n",
       "      <td>-3.948656</td>\n",
       "      <td>-5.728233</td>\n",
       "      <td>2.749578</td>\n",
       "      <td>3.217009</td>\n",
       "      <td>4.631289</td>\n",
       "      <td>6.615811</td>\n",
       "      <td>-2.037005</td>\n",
       "      <td>-1.550372</td>\n",
       "      <td>-1.908423</td>\n",
       "      <td>-2.582310</td>\n",
       "      <td>-1.258202</td>\n",
       "      <td>-2.497773</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.238189</td>\n",
       "      <td>0.220789</td>\n",
       "      <td>0.251084</td>\n",
       "      <td>0.483139</td>\n",
       "      <td>0.145572</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>1.955770</td>\n",
       "      <td>1.784399</td>\n",
       "      <td>0.148229</td>\n",
       "      <td>0.106867</td>\n",
       "      <td>0.542183</td>\n",
       "      <td>0.426717</td>\n",
       "      <td>0.540019</td>\n",
       "      <td>0.728101</td>\n",
       "      <td>0.370586</td>\n",
       "      <td>0.650584</td>\n",
       "      <td>1.722883</td>\n",
       "      <td>1.468238</td>\n",
       "      <td>0.342024</td>\n",
       "      <td>0.526934</td>\n",
       "      <td>0.276438</td>\n",
       "      <td>0.240730</td>\n",
       "      <td>0.290892</td>\n",
       "      <td>0.554917</td>\n",
       "      <td>0.173086</td>\n",
       "      <td>0.617256</td>\n",
       "      <td>2.064994</td>\n",
       "      <td>1.832695</td>\n",
       "      <td>0.200010</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.943192</td>\n",
       "      <td>2.273156</td>\n",
       "      <td>3.779405</td>\n",
       "      <td>4.966335</td>\n",
       "      <td>-2.944469</td>\n",
       "      <td>-3.943514</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-2.244316</td>\n",
       "      <td>-2.996533</td>\n",
       "      <td>-2.193731</td>\n",
       "      <td>-1.021096</td>\n",
       "      <td>-0.278392</td>\n",
       "      <td>1.917217</td>\n",
       "      <td>-6.369509</td>\n",
       "      <td>-6.095937</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-4.843174</td>\n",
       "      <td>-7.018910</td>\n",
       "      <td>2.070653</td>\n",
       "      <td>2.486572</td>\n",
       "      <td>3.920190</td>\n",
       "      <td>5.221436</td>\n",
       "      <td>-2.642684</td>\n",
       "      <td>-3.601235</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-13.815511</td>\n",
       "      <td>-1.854699</td>\n",
       "      <td>-2.899695</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.459589</td>\n",
       "      <td>2.783158</td>\n",
       "      <td>4.319752</td>\n",
       "      <td>6.040969</td>\n",
       "      <td>-2.449115</td>\n",
       "      <td>-2.734600</td>\n",
       "      <td>-3.521333</td>\n",
       "      <td>-3.896642</td>\n",
       "      <td>-1.820776</td>\n",
       "      <td>-2.852498</td>\n",
       "      <td>-1.459295</td>\n",
       "      <td>-0.181642</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>2.882004</td>\n",
       "      <td>-5.265076</td>\n",
       "      <td>-4.336671</td>\n",
       "      <td>-4.193723</td>\n",
       "      <td>-4.874619</td>\n",
       "      <td>-4.189095</td>\n",
       "      <td>-6.097714</td>\n",
       "      <td>2.565718</td>\n",
       "      <td>3.048325</td>\n",
       "      <td>4.432125</td>\n",
       "      <td>6.244749</td>\n",
       "      <td>-2.149006</td>\n",
       "      <td>-1.915963</td>\n",
       "      <td>-2.167180</td>\n",
       "      <td>-2.734446</td>\n",
       "      <td>-1.384696</td>\n",
       "      <td>-2.638617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.593013</td>\n",
       "      <td>2.935982</td>\n",
       "      <td>4.457134</td>\n",
       "      <td>6.311916</td>\n",
       "      <td>-2.344762</td>\n",
       "      <td>-2.379142</td>\n",
       "      <td>-2.788068</td>\n",
       "      <td>-3.396210</td>\n",
       "      <td>-1.719253</td>\n",
       "      <td>-2.788068</td>\n",
       "      <td>-1.126395</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>0.827241</td>\n",
       "      <td>3.199897</td>\n",
       "      <td>-5.054587</td>\n",
       "      <td>-3.889772</td>\n",
       "      <td>-3.653898</td>\n",
       "      <td>-4.516244</td>\n",
       "      <td>-3.977629</td>\n",
       "      <td>-5.748675</td>\n",
       "      <td>2.706048</td>\n",
       "      <td>3.235143</td>\n",
       "      <td>4.581492</td>\n",
       "      <td>6.531606</td>\n",
       "      <td>-2.030270</td>\n",
       "      <td>-1.551641</td>\n",
       "      <td>-1.484128</td>\n",
       "      <td>-2.303285</td>\n",
       "      <td>-1.265139</td>\n",
       "      <td>-2.525229</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.758743</td>\n",
       "      <td>3.081910</td>\n",
       "      <td>4.645352</td>\n",
       "      <td>6.662749</td>\n",
       "      <td>-2.250942</td>\n",
       "      <td>-2.037149</td>\n",
       "      <td>-2.034851</td>\n",
       "      <td>-2.603690</td>\n",
       "      <td>-1.631172</td>\n",
       "      <td>-2.716284</td>\n",
       "      <td>-0.736263</td>\n",
       "      <td>0.387980</td>\n",
       "      <td>1.211048</td>\n",
       "      <td>3.810876</td>\n",
       "      <td>-4.810228</td>\n",
       "      <td>-3.428055</td>\n",
       "      <td>-3.168896</td>\n",
       "      <td>-4.219228</td>\n",
       "      <td>-3.751606</td>\n",
       "      <td>-5.390871</td>\n",
       "      <td>2.933325</td>\n",
       "      <td>3.391820</td>\n",
       "      <td>4.831509</td>\n",
       "      <td>6.988413</td>\n",
       "      <td>-1.924149</td>\n",
       "      <td>-1.081460</td>\n",
       "      <td>-0.959981</td>\n",
       "      <td>-1.823870</td>\n",
       "      <td>-1.146018</td>\n",
       "      <td>-2.385098</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.336125</td>\n",
       "      <td>3.670715</td>\n",
       "      <td>5.239098</td>\n",
       "      <td>7.824446</td>\n",
       "      <td>-1.811554</td>\n",
       "      <td>-1.063052</td>\n",
       "      <td>-0.851440</td>\n",
       "      <td>-1.603456</td>\n",
       "      <td>-1.190728</td>\n",
       "      <td>-2.328518</td>\n",
       "      <td>1.055357</td>\n",
       "      <td>1.586169</td>\n",
       "      <td>3.090133</td>\n",
       "      <td>6.295635</td>\n",
       "      <td>-3.469583</td>\n",
       "      <td>-1.999522</td>\n",
       "      <td>-0.926341</td>\n",
       "      <td>-2.941434</td>\n",
       "      <td>-2.538941</td>\n",
       "      <td>-3.511906</td>\n",
       "      <td>3.584629</td>\n",
       "      <td>3.902780</td>\n",
       "      <td>5.526249</td>\n",
       "      <td>8.355615</td>\n",
       "      <td>-1.502379</td>\n",
       "      <td>0.056380</td>\n",
       "      <td>0.224742</td>\n",
       "      <td>-1.234432</td>\n",
       "      <td>-0.409774</td>\n",
       "      <td>-1.572624</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter   mean area  mean smoothness  \\\n",
       "count  569.000000   569.000000    569.000000      569.000000  569.000000        \n",
       "mean   2.619131     2.935269      4.489174        6.363185   -2.350210          \n",
       "std    0.238189     0.220789      0.251084        0.483139    0.145572          \n",
       "min    1.943192     2.273156      3.779405        4.966335   -2.944469          \n",
       "25%    2.459589     2.783158      4.319752        6.040969   -2.449115          \n",
       "50%    2.593013     2.935982      4.457134        6.311916   -2.344762          \n",
       "75%    2.758743     3.081910      4.645352        6.662749   -2.250942          \n",
       "max    3.336125     3.670715      5.239098        7.824446   -1.811554          \n",
       "\n",
       "       mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "count  569.000000        569.000000      569.000000           569.000000      \n",
       "mean  -2.380518         -3.092911       -3.553578            -1.719430        \n",
       "std    0.494459          1.955770        1.784399             0.148229        \n",
       "min   -3.943514         -13.815511      -13.815511           -2.244316        \n",
       "25%   -2.734600         -3.521333       -3.896642            -1.820776        \n",
       "50%   -2.379142         -2.788068       -3.396210            -1.719253        \n",
       "75%   -2.037149         -2.034851       -2.603690            -1.631172        \n",
       "max   -1.063052         -0.851440       -1.603456            -1.190728        \n",
       "\n",
       "       mean fractal dimension  radius error  texture error  perimeter error  \\\n",
       "count  569.000000              569.000000    569.000000     569.000000        \n",
       "mean  -2.773718               -1.065554      0.104874       0.889575          \n",
       "std    0.106867                0.542183      0.426717       0.540019          \n",
       "min   -2.996533               -2.193731     -1.021096      -0.278392          \n",
       "25%   -2.852498               -1.459295     -0.181642       0.473747          \n",
       "50%   -2.788068               -1.126395      0.102557       0.827241          \n",
       "75%   -2.716284               -0.736263      0.387980       1.211048          \n",
       "max   -2.328518                1.055357      1.586169       3.090133          \n",
       "\n",
       "       area error  smoothness error  compactness error  concavity error  \\\n",
       "count  569.000000  569.000000        569.000000         569.000000        \n",
       "mean   3.379864   -5.028600         -3.879864          -3.958432          \n",
       "std    0.728101    0.370586          0.650584           1.722883          \n",
       "min    1.917217   -6.369509         -6.095937          -13.815511         \n",
       "25%    2.882004   -5.265076         -4.336671          -4.193723          \n",
       "50%    3.199897   -5.054587         -3.889772          -3.653898          \n",
       "75%    3.810876   -4.810228         -3.428055          -3.168896          \n",
       "max    6.295635   -3.469583         -1.999522          -0.926341          \n",
       "\n",
       "       concave points error  symmetry error  fractal dimension error  \\\n",
       "count  569.000000            569.000000      569.000000                \n",
       "mean  -4.741481             -3.948656       -5.728233                  \n",
       "std    1.468238              0.342024        0.526934                  \n",
       "min   -13.815511            -4.843174       -7.018910                  \n",
       "25%   -4.874619             -4.189095       -6.097714                  \n",
       "50%   -4.516244             -3.977629       -5.748675                  \n",
       "75%   -4.219228             -3.751606       -5.390871                  \n",
       "max   -2.941434             -2.538941       -3.511906                  \n",
       "\n",
       "       worst radius  worst texture  worst perimeter  worst area  \\\n",
       "count  569.000000    569.000000     569.000000       569.000000   \n",
       "mean   2.749578      3.217009       4.631289         6.615811     \n",
       "std    0.276438      0.240730       0.290892         0.554917     \n",
       "min    2.070653      2.486572       3.920190         5.221436     \n",
       "25%    2.565718      3.048325       4.432125         6.244749     \n",
       "50%    2.706048      3.235143       4.581492         6.531606     \n",
       "75%    2.933325      3.391820       4.831509         6.988413     \n",
       "max    3.584629      3.902780       5.526249         8.355615     \n",
       "\n",
       "       worst smoothness  worst compactness  worst concavity  \\\n",
       "count  569.000000        569.000000         569.000000        \n",
       "mean  -2.037005         -1.550372          -1.908423          \n",
       "std    0.173086          0.617256           2.064994          \n",
       "min   -2.642684         -3.601235          -13.815511         \n",
       "25%   -2.149006         -1.915963          -2.167180          \n",
       "50%   -2.030270         -1.551641          -1.484128          \n",
       "75%   -1.924149         -1.081460          -0.959981          \n",
       "max   -1.502379          0.056380           0.224742          \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "count  569.000000            569.000000      569.000000                \n",
       "mean  -2.582310             -1.258202       -2.497773                  \n",
       "std    1.832695              0.200010        0.195784                  \n",
       "min   -13.815511            -1.854699       -2.899695                  \n",
       "25%   -2.734446             -1.384696       -2.638617                  \n",
       "50%   -2.303285             -1.265139       -2.525229                  \n",
       "75%   -1.823870             -1.146018       -2.385098                  \n",
       "max   -1.234432             -0.409774       -1.572624                  \n",
       "\n",
       "           target  \n",
       "count  569.000000  \n",
       "mean   0.627417    \n",
       "std    0.483918    \n",
       "min    0.000000    \n",
       "25%    0.000000    \n",
       "50%    1.000000    \n",
       "75%    1.000000    \n",
       "max    1.000000    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    296.000000\n",
       "mean     0.500000  \n",
       "std      0.500847  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.500000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets load a dataset for classification:\n",
    "data = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "\n",
    "num_features = len(data.feature_names)\n",
    "# some preprocessing e.g. taking log\n",
    "for each_col in data.feature_names:\n",
    "    df[each_col] = np.log(np.maximum(df[each_col], 10**-6))\n",
    "df['target'] = data.target\n",
    "print('processed data')\n",
    "display(df.describe())\n",
    "\n",
    "# balance the class and do train/test split\n",
    "df = pd.concat([df[df.target == 0].iloc[:212], df[df.target == 1].iloc[:212]])\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(df[data.feature_names], df.target, test_size=0.3, random_state=0, stratify=df.target)\n",
    "\n",
    "# balanced?\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.88059701 0.91803279]\n",
      "recall: [0.921875 0.875   ]\n",
      "fbeta_score: [0.90076336 0.896     ]\n",
      "test data split: [64 64]\n"
     ]
    }
   ],
   "source": [
    "# lets feed the data in a small network:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_neurons = [8, 5, 3, 1] # in each layer\n",
    "n_in = len(data.feature_names) # input size\n",
    "n_mb = 8 # mini batch size\n",
    "n_mc = 2000 # number of mc samples per input array\n",
    "n_epoch = 100\n",
    "\n",
    "LAYER = 0  # which layer to print out in this experiment\n",
    "\n",
    "y_train_mb = np.reshape(y_train.values, [-1, n_mb, 1])\n",
    "x_train_mb = np.reshape(x_train.values, [-1, n_mb, n_in])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    mc_mlp = MCMLP(n_in, n_neurons, n_mc, learning_rate=0.005, mc=True)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for n in range(n_epoch):\n",
    "        for each_x_mb, each_y_mb in zip(x_train_mb, y_train_mb): \n",
    "            (mu_, tiled_mu_, sig_sqrt_, sig_,\n",
    "             eps_, weights_, weights_mb_, hidden_, kl_, output_p_,\n",
    "             l_likelihood_, kl_sum_, cost_,\n",
    "             training_op_) = sess.run([mc_mlp.mu[LAYER], mc_mlp.tiled_mu[LAYER],\n",
    "                                                         mc_mlp.sig_sqrt[LAYER], mc_mlp.sig[LAYER],\n",
    "                                                         mc_mlp.eps[LAYER], mc_mlp.weights[LAYER],\n",
    "                                                         mc_mlp.weights_mb[LAYER], mc_mlp.hidden[LAYER],\n",
    "                                                         mc_mlp.kl[LAYER],\n",
    "                                                         mc_mlp.output_p, mc_mlp.l_likelihood, \n",
    "                                                         mc_mlp.kl_sum, mc_mlp.cost,\n",
    "                                                         mc_mlp.training_op],\n",
    "                                                        {mc_mlp.x: each_x_mb, \n",
    "                                                         mc_mlp.y: each_y_mb})\n",
    "#         print(n, l_likelihood_, kl_sum_, cost_)\n",
    "    \n",
    "    # evaluate\n",
    "    y_test_arr = y_test.values  # [:8]\n",
    "    x_test_arr = x_test.values  # [:8]\n",
    "    test_output_ = sess.run(mc_mlp.output, {mc_mlp.x: x_test_arr})\n",
    "    (precision, recall, \n",
    "     fbeta_score, split) = precision_recall_fscore_support(y_test_arr, test_output_)\n",
    "    print('precision: {}'.format(precision))\n",
    "    print('recall: {}'.format(recall))\n",
    "    print('fbeta_score: {}'.format(fbeta_score))\n",
    "    print('test data split: {}'.format(split))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "Peter M. Lee, \"Bayesian Statistics - an introduction\", Hodder Arnold. \n",
    "\n",
    "Yarin Gal, \"Uncertainty in Deep Learning,\" PhD Thesis. Uni. Cambridge, 2016.\n",
    "\n",
    "F. J. R. Ruiz, M. K. Titsias, D. M. Blei, \"[The Generalized Reparameterization Gradient](https://arxiv.org/abs/1610.02287)\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
